{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33253039",
   "metadata": {},
   "source": [
    "Read data and prepare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ee21aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from gensim.models import Word2Vec\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f460d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global constants\n",
    "tokenizer = Tokenizer()\n",
    "vectorizer = TfidfVectorizer()\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "EMBEDDING_DIM = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "18d1252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_classes(dataset):\n",
    "    dataset['label']=[1 if x==\"true\"or x==\"mostly-true\" else 0 for x in dataset[1]]\n",
    "    \n",
    "    #Dealing with empty datapoints for metadata columns - subject, speaker, job, state,affiliation, context\n",
    "    meta = []\n",
    "    for i in range(len(dataset)):\n",
    "      subject = dataset[3][i]\n",
    "      if subject == 0:\n",
    "          subject = 'None'\n",
    "\n",
    "      speaker =  dataset[4][i]\n",
    "      if speaker == 0:\n",
    "          speaker = 'None'\n",
    "\n",
    "      job =  dataset[5][i]\n",
    "      if job == 0:\n",
    "          job = 'None'\n",
    "\n",
    "      state =  dataset[6][i]\n",
    "      if state == 0:\n",
    "          state = 'None'\n",
    "\n",
    "      affiliation =  dataset[7][i]\n",
    "      if affiliation == 0:\n",
    "          affiliation = 'None'\n",
    "\n",
    "      context =  dataset[13][i]\n",
    "      if context == 0 :\n",
    "          context = 'None'\n",
    "\n",
    "      meta.append(str(subject) + ' ' + str(speaker) + ' ' + str(job) + ' ' + str(state) + ' ' + str(affiliation) + ' ' + str(context)) #combining all the meta data columns into a single column\n",
    "  \n",
    "    #Adding cleaned and combined metadata column to the dataset\n",
    "    dataset[14] = meta\n",
    "    dataset[\"sentence\"] = dataset[14].astype('str')+\" \"+dataset[2]\n",
    "    \n",
    "    #Dropping unwanted columns\n",
    "    dataset = dataset.drop(labels=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14] ,axis=1)\n",
    "    dataset.dropna()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "247fb6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(dataset):\n",
    "    preprocessed_texts = []\n",
    "    for text in dataset:\n",
    "        # convert to lowercase\n",
    "        text = text.lower()    \n",
    "        # tokenize text\n",
    "        tokens = word_tokenize(text)\n",
    "        # remove punctuation and irrelevant characters\n",
    "        filtered_tokens = [token for token in tokens if token.isalnum()]\n",
    "        # remove stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_tokens = [token for token in filtered_tokens if not token in stop_words]\n",
    "        # lemmatize tokens\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        filtered_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "        # stem tokens\n",
    "        stemmer = PorterStemmer()\n",
    "        filtered_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "        # join tokens back into string\n",
    "        preprocessed_text = ' '.join(filtered_tokens)\n",
    "        preprocessed_texts.append(preprocessed_text)\n",
    "    return preprocessed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "938924f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing_without_stem(dataset):\n",
    "    preprocessed_texts = []\n",
    "    for text in dataset:\n",
    "        text = text.lower()\n",
    "        text = str(text).replace(r'\\.\\.+','.') #replace multiple periods with a single one\n",
    "        text = str(text).replace(r'\\.','.') #replace periods with a single one\n",
    "        text = str(text).replace(r'\\s\\s+',' ') #replace multiple white space with a single one\n",
    "        text = str(text).replace(\"\\n\", \"\") #removing line breaks\n",
    "        \n",
    "        # tokenize text\n",
    "        tokens = word_tokenize(text)\n",
    "        # remove punctuation and irrelevant characters\n",
    "        filtered_tokens = [token for token in tokens if token.isalnum()]\n",
    "        # remove stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_tokens = [token for token in filtered_tokens if not token in stop_words]\n",
    "        \n",
    "        # lemmatize tokens\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        filtered_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "        # join tokens back into string\n",
    "        preprocessed_text = ' '.join(filtered_tokens)\n",
    "        preprocessed_texts.append(preprocessed_text)\n",
    "    return preprocessed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "15e0ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_preprocessing(data_train, data_valid, data_test,preproccess_type=\"all\"):\n",
    "    if preproccess_type==\"all\":\n",
    "        data_train['sentence'] = data_preprocessing(data_train['sentence'])\n",
    "        data_valid['sentence'] = data_preprocessing(data_valid['sentence'])\n",
    "        data_test['sentence'] = data_preprocessing(data_test['sentence'])\n",
    "        print(\"data_preprocessing done!\")\n",
    "    else:\n",
    "        data_train['sentence'] = data_preprocessing_without_stem(data_train['sentence'])\n",
    "        data_valid['sentence'] = data_preprocessing_without_stem(data_valid['sentence'])\n",
    "        data_test['sentence'] = data_preprocessing_without_stem(data_test['sentence'])\n",
    "        print(\"data_preprocessing_without_stem done!\")\n",
    "    data_train.head(5)\n",
    "    return data_train, data_valid, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ed8f9682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_tokenizer(X_train, X_test, data_set_all):\n",
    "#     tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data_set_all['sentence'])\n",
    "    \n",
    "    vocab_size =len(tokenizer.word_index) + 1\n",
    "    \n",
    "    X_train = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test = tokenizer.texts_to_sequences(X_test)\n",
    "    return X_train,X_test, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fa1c3951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(X_train, X_test):\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "    \n",
    "    vocab_size = len(vectorizer.vocabulary_) + 1  # Vocabulary size for word embedding\n",
    "    \n",
    "    # Prepare the input sequences for the LSTM model\n",
    "    X_train_sequences = pad_sequences(X_train_vectorized.toarray(),\n",
    "                                            maxlen=MAX_SEQUENCE_LENGTH, truncating='post', padding='post')\n",
    "    X_test_sequences = pad_sequences(X_test_vectorized.toarray(),\n",
    "                                            maxlen=MAX_SEQUENCE_LENGTH, truncating='post', padding='post')\n",
    "    print(\"Vectorizing done!\")\n",
    "    return X_train_vectorized, X_test_vectorized, vocab_size, X_train_sequences, X_test_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "78ae4c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_vectorizer(X_train, X_test, data_set_all):\n",
    "    \n",
    "    word2vec_model = Word2Vec(sentences=[text.split() for text in data_set_all['sentence']],\n",
    "                              vector_size=EMBEDDING_DIM, window=5, min_count=1, workers=4)\n",
    "    vectorized_data = []\n",
    "\n",
    "    for texts in [X_train, X_test]:\n",
    "        data_vectors = []\n",
    "        for text in texts:\n",
    "            text_vectors = [word2vec_model.wv[word] for word in text if word in word2vec_model.wv]\n",
    "            if len(text_vectors) > 0:\n",
    "                data_vectors.append(np.mean(text_vectors, axis=0))\n",
    "            else:\n",
    "                data_vectors.append(np.zeros(word2vec_model.vector_size))\n",
    "        vectorized_data.append(data_vectors)\n",
    "\n",
    "    X_train_vectors, X_test_vectors = vectorized_data\n",
    "    \n",
    "    # Create embedding matrix\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if word in word2vec_model.wv:\n",
    "            embedding_matrix[i] = word2vec_model.wv[word]\n",
    "            \n",
    "    # Prepare the input sequences for the LSTM model\n",
    "    X_train_sequences = pad_sequences(X_train_vectors,\n",
    "                                            maxlen=MAX_SEQUENCE_LENGTH, truncating='post', padding='post')\n",
    "    X_test_sequences = pad_sequences(X_test_vectors,\n",
    "                                            maxlen=MAX_SEQUENCE_LENGTH, truncating='post', padding='post')\n",
    "    \n",
    "    print(\"Word2Vec vectorizing done!\")\n",
    "    return X_train_vectors, X_test_vectors, embedding_matrix,X_train_sequences,X_test_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c2c2a19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_vectorize(data_set, data_train, data_test):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data_set['sentence'])\n",
    "    \n",
    "    # Convert text to sequences\n",
    "    X_train = tokenizer.texts_to_sequences(data_train['sentence'])\n",
    "    X_test = tokenizer.texts_to_sequences(data_test['sentence'])\n",
    "    \n",
    "    \n",
    "    # Pad sequences\n",
    "    X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    # Prepare target labels\n",
    "    y_train = data_train['label'].values\n",
    "    y_test = data_test['label'].values\n",
    "    \n",
    "    # Create Word2Vec embeddings\n",
    "    word2vec = Word2Vec(sentences=[text.split() for text in data_set['sentence']],\n",
    "                        vector_size=EMBEDDING_DIM, window=5, min_count=1, workers=4)\n",
    "\n",
    "    # Create embedding matrix\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if word in word2vec.wv:\n",
    "            embedding_matrix[i] = word2vec.wv[word]\n",
    "    vocab_size =len(tokenizer.word_index) + 1\n",
    "    return embedding_matrix,X_train, y_train, X_test, y_test,vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e9b4f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def word2vec_embed(X_train,X_valid,X_test):\n",
    "    \n",
    "#     combined_data = X_train + X_valid + X_test\n",
    "\n",
    "#     # Word2Vec embedding\n",
    "#     word2vec = Word2Vec(sentences=combined_data, vector_size=EMBEDDING_DIM, window=5, min_count=1, workers=4)\n",
    "\n",
    "#     # Split the embedded data back into training, validation, and test sets\n",
    "#     X_train_word2vec = [word2vec.wv[phrase] for phrase in X_train]\n",
    "#     X_valid_word2vec = [word2vec.wv[phrase] for phrase in X_valid]\n",
    "#     X_test_word2vec = [word2vec.wv[phrase] for phrase in X_test]\n",
    "\n",
    "#     # Create embedding matrix\n",
    "#     embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, EMBEDDING_DIM))\n",
    "#     for word, i in tokenizer.word_index.items():\n",
    "#         if word in word2vec.wv:\n",
    "#             embedding_matrix[i] = word2vec.wv[word]\n",
    "#     print(\"Word2Vec embedding done!\")\n",
    "#     return embedding_matrix,X_train_word2vec,X_valid_word2vec,X_test_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "79f1a097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversampling_data(X,y):\n",
    "    smote = SMOTE()\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    \n",
    "    X_resampled, y_resampled = shuffle(X_resampled, y_resampled, random_state=42)\n",
    "\n",
    "    print(\"Before SMOTE:\", Counter(y))\n",
    "    print(\"After SMOTE:\", Counter(y_resampled))\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "78ff598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_vectorize_data(data_set):\n",
    "    # Tokenization\n",
    "    tokenizer.fit_on_texts(your_text_data)\n",
    "    sequences = tokenizer.texts_to_sequences(your_text_data)\n",
    "\n",
    "    # Word2Vec\n",
    "    word2vec = Word2Vec(sentences=sequences, vector_size=EMBEDDING_DIM, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a2e07a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    data_train = pd.read_csv(\"sample_data/train.tsv\", sep=\"\\t\", header=None)\n",
    "    data_valid = pd.read_csv(\"sample_data/valid.tsv\", sep=\"\\t\", header=None)\n",
    "    data_test = pd.read_csv(\"sample_data/test.tsv\", sep=\"\\t\", header=None)\n",
    "    data_train.head(3)\n",
    "    return data_train, data_valid, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d05b2727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_length(data_set):\n",
    "    #Analyzing length of sentences in training data to decide on MAX_LENGTH variable, which is required for mlp and deep_leaner\n",
    "    sent_len = []\n",
    "    for sent in data_set['sentence']:\n",
    "      sent_len.append(len(sent))\n",
    "\n",
    "    fig = plt.figure(figsize =(10, 7))\n",
    "    plt.boxplot(sent_len)\n",
    "    plt.show()\n",
    "\n",
    "    sent_len = [i for i in sent_len if i<=500] #Excluding the outliers\n",
    "    fig2 = plt.figure(figsize =(10, 7))\n",
    "    plt.hist(sent_len, 5)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
