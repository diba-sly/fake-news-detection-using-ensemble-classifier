{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "772c7608",
   "metadata": {},
   "source": [
    "XGBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89ac1416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix, auc, roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9571ff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_search(X_train, y_train, X_test, y_test):\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "                        \n",
    "    # Define the parameter grid to search over\n",
    "    param_grid={\n",
    "    'objective': ['binary:logistic'],\n",
    "    'eval_metric': ['logloss'],\n",
    "    'eta': [0.01, 0.1],  # learning rate\n",
    "    'max_depth': [15, 20, 25],  # maximum depth of a tree\n",
    "    'subsample': [0.2, 0.5],  # subsample ratio of the training instances\n",
    "    'colsample_bytree': [0.5, 0.8]  # subsample ratio of columns when constructing each tree\n",
    "    }\n",
    "\n",
    "    num_rounds = 300  # number of boosting rounds (iterations)\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='f1', cv=10)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best Parameters found by Grid Search:\", best_params)\n",
    "\n",
    "    model = xgb.train(best_params, dtrain, num_rounds)\n",
    "    y_pred = model.predict(dtest)\n",
    "    y_pred_binary = [1 if p >= 0.5 else 0 for p in y_pred]\n",
    "\n",
    "    return y_pred_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bbea970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(X_train, y_train, X_test, y_test, params, rounds=300, threshold=0.4):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "    \n",
    "#     params = {\n",
    "#     'objective': 'binary:logistic',  # or 'multi:softmax' for multi-class classification\n",
    "#     'eval_metric': 'logloss',  # or other appropriate evaluation metric\n",
    "#     'eta': 0.01,  # learning rate\n",
    "#     'max_depth': 20,  # maximum depth of a tree\n",
    "#     'subsample': 0.2,  # subsample ratio of the training instances\n",
    "#     'colsample_bytree': 0.8  # subsample ratio of columns when constructing each tree\n",
    "#     }\n",
    "    \n",
    "    num_rounds = rounds  # number of boosting rounds (iterations)     \n",
    "    model = xgb.train(params, dtrain, num_rounds)\n",
    "    \n",
    "    \n",
    "    y_pred = model.predict(dtest)\n",
    "    y_train_valid_pred = model.predict(dtrain)\n",
    "    \n",
    "    xgb_pred_binary = [1 if p >= threshold else 0 for p in y_pred]\n",
    "    xgb_train_valid_pred_binary = [1 if p >= threshold else 0 for p in y_train_valid_pred]\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(\"Execution time of XGBoost: {:.2f} seconds\".format(execution_time))\n",
    "    \n",
    "    return xgb_pred_binary, xgb_train_valid_pred_binary, y_pred, y_train_valid_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eafa9b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_cross_validation(X_train, y_train, params, rounds=300, fold=5):\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "   \n",
    "#     params = {\n",
    "#     'objective': 'binary:logistic',  # or 'multi:softmax' for multi-class classification\n",
    "#     'eval_metric': 'logloss',  # or other appropriate evaluation metric\n",
    "#     'eta': 0.01,  # learning rate\n",
    "#     'max_depth': 20,  # maximum depth of a tree\n",
    "#     'subsample': 0.2,  # subsample ratio of the training instances\n",
    "#     'colsample_bytree': 0.8  # subsample ratio of columns when constructing each tree\n",
    "#     }\n",
    "    \n",
    "    num_rounds = rounds  # number of boosting rounds (iterations)     \n",
    "    model = xgb.train(params, dtrain, num_rounds)\n",
    "    \n",
    "    kfold = KFold(n_splits=fold, shuffle=True, random_state=42)\n",
    "    \n",
    "    def f1_scoring(estimator, X, y):\n",
    "        y_pred = estimator.predict(xgb.DMatrix(X))\n",
    "        return f1_score(y, y_pred)\n",
    "    \n",
    "    results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=f1_scoring)\n",
    "    \n",
    "    print(f'F1 Score: {results.mean():.4f} (+/- {results.std():.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "076daf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_metrics(xgb_predictions, y_test):\n",
    "    accuracy = sum(xgb_predictions == y_test) / len(y_test)\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_test,\n",
    "                                                                         xgb_predictions, average='binary')\n",
    "    print(\"XGBoost - accuracy:\", accuracy)\n",
    "    print(\"XGBoost - Precision:\", precision)\n",
    "    print(\"XGBoost - Recall:\", recall)\n",
    "    print(\"XGBoost - F-score:\", fscore)\n",
    "    return accuracy, precision, recall, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1980ae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_confusion_matrix_plot(xgb_predictions, y_test):\n",
    "    cm = confusion_matrix(y_test, xgb_predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', cbar=False)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix - XGBoost')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95ec3ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_roc_plot(xgb_predictions, y_test):\n",
    "    # Compute the true positive rate (tpr) and false positive rate (fpr) using roc_curve\n",
    "    xgb_fpr, xgb_tpr, xgb_thresholds = roc_curve(y_test, xgb_predictions)\n",
    "    xgb_roc_auc = auc(xgb_fpr, xgb_tpr)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure()\n",
    "    plt.plot(xgb_fpr, xgb_tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % xgb_roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) - XGBoost')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89121199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_f1_threshold_plot(y_test, y_pred):\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    f1_scores = [f1_score(y_test, y_pred > threshold) for threshold in thresholds]\n",
    "    # Plot F1 score vs. threshold\n",
    "    plt.figure()\n",
    "    plt.plot(thresholds, f1_scores)\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('F1 Score vs. Threshold - XGBoost')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6561902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_learning_plot():\n",
    "    # Plot learning curve\n",
    "    train_sizes, train_scores, test_scores = learning_curve(model, X_train, y_train, scoring='f1', cv=5)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, np.mean(train_scores, axis=1), label='Training score')\n",
    "    plt.plot(train_sizes, np.mean(test_scores, axis=1), label='Cross-validation score')\n",
    "    plt.xlabel('Number of training examples')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Learning Curve - XGBoost')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
